{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662001d8",
   "metadata": {},
   "source": [
    "## Binary Classification Neural Net Sigmoid Batch Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7fde414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "from scipy.stats import truncnorm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe776d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and prepare data\n",
    "\n",
    "train_data=np.loadtxt(\"data/assignment/overhead_mnist_train.csv\",delimiter=\",\")\n",
    "test_data=np.loadtxt(\"data/assignment/overhead_mnist_test.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9bf2091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the feature vectors from the training data.\n",
    "X_train=train_data[:,1:]\n",
    "\n",
    "# Extracting the labels from the training data.\n",
    "y_train=train_data[:,0]\n",
    "\n",
    "# Extracting the feature vectors from the testing data.\n",
    "X_test=test_data[:,1:]\n",
    "\n",
    "# Extracting the labels from the testing data.\n",
    "y_test=test_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd2b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shapes of the feature and label datasets for both training and testing.\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n",
    "\n",
    "# Calculating a normalization factor\n",
    "normalization_factor=0.99/X_train.max()\n",
    "\n",
    "# Applying the normalization to the training features.\n",
    "X_train=X_train * normalization_factor + 0.01\n",
    "\n",
    "# Applying the same normalization to the testing features.\n",
    "X_test=X_test * normalization_factor + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49cc0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean array where entries are True for instances of class 7 in the training and testing labels, \n",
    "#then convert to float (1.0 for \n",
    "class7_train=([7]==y_train).astype(float)\n",
    "class7_test=([7]==y_test).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f453a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the class 7 training and testing labels to a 2D array with a single column.\n",
    "class7_train=class7_train.reshape(-1,1)\n",
    "class7_test=class7_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a8b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid activation function using numpy for vectorized operations, to be applied element-wise.\n",
    "\n",
    "@np.vectorize\n",
    "def sigmoid(a):\n",
    "    return 1/(1+np.e**(-a))\n",
    "\n",
    "# Define a function to generate values from a truncated normal distribution \n",
    "#within specified bounds, mean, and standard deviation.\n",
    "\n",
    "def truncated_normal(low,upp,mean,sd):\n",
    "    return truncnorm((low-mean)/sd,(upp-mean)/sd,scale=sd,loc=mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7fab7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3506da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    \n",
    "    def __init__(self,no_inputs, hidden_layers, no_outputs,bias,learning_rate):\n",
    "        \n",
    "        self.no_inputs=no_inputs\n",
    "        self.hidden_layers=hidden_layers\n",
    "        self.no_outputs=no_outputs\n",
    "        self.bias = bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.structure = [no_inputs] + hidden_layers + [no_outputs]\n",
    "        self.create_weight()\n",
    "    \n",
    "    \n",
    "    def create_weight(self):\n",
    "        bias_node = 1 if self.bias else 0\n",
    "        self.len_network = len(self.structure)\n",
    "        layer_index = 0\n",
    "        self.weights = []\n",
    "        \n",
    "        while layer_index < self.len_network - 1:\n",
    "            nodes_out = self.structure[layer_index + 1]\n",
    "            nodes_in = self.structure[layer_index]\n",
    "            n = nodes_out * (nodes_in + bias_node)\n",
    "            rad = 1 / np.sqrt(nodes_in)\n",
    "            X=truncated_normal(low=-rad,upp=rad,mean=0,sd=1)\n",
    "            wm = X.rvs(n).reshape((nodes_out, nodes_in + 1))\n",
    "            self.weights.append(wm)\n",
    "            layer_index += 1\n",
    "        return self.weights\n",
    "    \n",
    "    def alternative_weight_init(self):\n",
    "    \n",
    "    \n",
    "    #This weight initialization demonstrates an alternative approach to initializing weights for the neural network\n",
    "    \n",
    "    #Directly alligned with the pseudocode in the template\n",
    "    \n",
    "    # TO-DO:\n",
    "        #for layer in range(len(hidden_layers)):\n",
    "            #no_nodes = hidden_layers[layer]\n",
    "            # no_inputs_to_layer = ??\n",
    "            # initialise weight matrix of shape: (no_nodes, no_inputs_to_layer)\n",
    "  \n",
    "    \n",
    "        bias_node=1 if self.bias else 0\n",
    "    \n",
    "        self.weights_new=[]\n",
    "    \n",
    "        for layer in range(len(self.hidden_layers)+1):\n",
    "        \n",
    "            nodes_in=self.hidden_layers[layer-1]\n",
    "        \n",
    "            if layer==0:\n",
    "                nodes_in=self.no_inputs+bias_node\n",
    "            \n",
    "            if layer==3:\n",
    "                nodes_out=self.no_outputs\n",
    "            else:\n",
    "                nodes_out=self.hidden_layers[layer]\n",
    "\n",
    "            n = nodes_out * (nodes_in + bias_node)\n",
    "\n",
    "            rad = 1 / np.sqrt(nodes_in)\n",
    "\n",
    "            X=truncated_normal(low=-rad,upp=rad,mean=0,sd=1)\n",
    "\n",
    "            wm = X.rvs(n).reshape((nodes_out, nodes_in + 1))\n",
    "\n",
    "            self.weights_new.append(wm)\n",
    "\n",
    "            layer += 1\n",
    "        \n",
    "        return self.weights_new\n",
    "    \n",
    "    \n",
    "    def train_batch(self, X_train, labels, epochs, batch_size):\n",
    "        self.combined_weights = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_train_shuffled = X_train[indices]\n",
    "            labels_shuffled = labels[indices]\n",
    "\n",
    "            for start_idx in range(0, X_train.shape[0], batch_size):\n",
    "                end_idx = min(start_idx + batch_size, X_train.shape[0])\n",
    "                batch_x = X_train_shuffled[start_idx:end_idx]\n",
    "                batch_y = labels_shuffled[start_idx:end_idx]\n",
    "\n",
    "                result_vector = [batch_x.T]\n",
    "                layer_index = 0\n",
    "\n",
    "                while layer_index < self.len_network - 1:\n",
    "                    input_vector = result_vector[-1]\n",
    "\n",
    "                    if self.bias:\n",
    "                        bias_terms = np.ones((1, input_vector.shape[1])) * self.bias\n",
    "                        input_vector = np.concatenate((input_vector, bias_terms), axis=0)\n",
    "                        result_vector[-1] = input_vector\n",
    "                    \n",
    "                    output_z = np.dot(self.weights[layer_index], input_vector)\n",
    "                    output_activation = activate(output_z)\n",
    "                    result_vector.append(output_activation)\n",
    "                    layer_index += 1\n",
    "\n",
    "                output_error = batch_y.T - result_vector[-1]\n",
    "                backward_index = self.len_network - 1\n",
    "\n",
    "                while backward_index > 0:\n",
    "                    output_activation = result_vector[backward_index]\n",
    "                    input_activation = result_vector[backward_index - 1]\n",
    "                    gradient = np.dot(output_error, input_activation.T) / batch_x.shape[0]\n",
    "                    self.weights[backward_index - 1] += self.learning_rate * gradient\n",
    "                    output_error = np.dot(self.weights[backward_index - 1].T, output_error)\n",
    "\n",
    "                    if self.bias:\n",
    "                        output_error = output_error[:-1, :]\n",
    "                    backward_index -= 1\n",
    "            \n",
    "            self.combined_weights.append(self.weights.copy())\n",
    "        return self.combined_weights\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        result_vector = [input_vector]\n",
    "        layer_index = 0\n",
    "\n",
    "        while layer_index < self.len_network - 1:\n",
    "            input_vector = result_vector[-1]\n",
    "\n",
    "            if self.bias:\n",
    "                input_vector = np.concatenate((input_vector, [[self.bias]]))\n",
    "                result_vector[-1] = input_vector\n",
    "\n",
    "            output_z = np.dot(self.weights[layer_index], input_vector)\n",
    "            output_activation = activate(output_z)\n",
    "            result_vector.append(output_activation)\n",
    "            layer_index += 1\n",
    "        \n",
    "        return output_activation\n",
    "        \n",
    "    def accuracy(self, data_array, labels):\n",
    "        correct, wrong = 0, 0\n",
    "\n",
    "        for i in range(len(data_array)):\n",
    "            predicted = self.predict(data_array[i])\n",
    "            predicted = np.where(predicted[0][0] >= 0.5, 1, 0)\n",
    "            actual_label = int(labels[i][0])\n",
    "\n",
    "            if predicted == actual_label:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "\n",
    "        accuracy = correct / (correct + wrong)\n",
    "        return accuracy\n",
    "\n",
    "    def confusion_matrix(self, data_array, labels):\n",
    "        cm = np.zeros((2, 2), int)\n",
    "\n",
    "        for i in range(len(data_array)):\n",
    "            predicted = self.predict(data_array[i])\n",
    "            predicted = np.where(predicted >= 0.15, 1, 0)\n",
    "            actual_label = int(labels[i][0])\n",
    "            cm[actual_label, predicted] += 1\n",
    "\n",
    "        return cm\n",
    "\n",
    "    def precision_recall(self, label, confusion_matrix):\n",
    "        \n",
    "        true_positives=confusion_matrix[0,0]+confusion_matrix[1,1]\n",
    "        false_positives=confusion_matrix[0,1]\n",
    "        false_negatives=confusion_matrix[1,0]\n",
    "        \n",
    "        precision=true_positives/(true_positives+false_positives)\n",
    "        recall=true_positives/(true_positives+false_negatives)\n",
    "      \n",
    "        print(\"Accuracy:\\t\"+str(accuracy))\n",
    "        print(\"Precision:\\t\"+str(precision))\n",
    "        print(\"Recall:\\t\"+str(recall))\n",
    "        print(confusion_matrix)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f552bbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 285.80098581314087 seconds\n"
     ]
    }
   ],
   "source": [
    "start_binary_neural_training_time = time.time()\n",
    "\n",
    "binary_exclusive=ANN(784,[100,80,50],1,1,0.5) #an instance of a class exclusively built for binary classification\n",
    "\n",
    "bi_weights=binary_exclusive.train_batch(X_train,class7_train,100,64)\n",
    "\n",
    "bi_accuracy=binary_exclusive.accuracy(X_test, class7_test)\n",
    "\n",
    "end_binary_neural_training_time = time.time()\n",
    "\n",
    "time_taken_binary_training=end_binary_neural_training_time - start_binary_neural_training_time\n",
    "\n",
    "print(f\"Execution time: {time_taken_binary_training} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9c07652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurcy of binary neural network with sigmoid is 0.895774647887324\n"
     ]
    }
   ],
   "source": [
    "bi_accuracy=binary_exclusive.accuracy(X_test, class7_test)\n",
    "print(f\"accurcy of binary neural network with sigmoid is {bi_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc66c45",
   "metadata": {},
   "source": [
    "## Updating the ANN implement to allow binary or multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "426f66c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training labels to a two-dimensional array with a single column, preparing it for one-hot encoding.\n",
    "y_train_full_encoded = y_train.reshape(-1, 1)\n",
    "\n",
    "# Reshape the testing labels similarly to the training labels, ensuring consistency in data structure.\n",
    "y_test_full_encoded = y_test.reshape(-1, 1)\n",
    "\n",
    "# Create an array representing class labels for a classification task with 10 classes (0 through 9).\n",
    "class_encoder = np.arange(10)\n",
    "\n",
    "# Perform one-hot encoding on the training labels. For each label in `y_train_full_encoded`, create an array where the index\n",
    "y_train_encoded = (class_encoder == y_train_full_encoded).astype(float)\n",
    "\n",
    "# Apply the same one-hot encoding process to the testing labels, transforming them into a binary matrix representation\n",
    "y_test_encoded = (class_encoder == y_test_full_encoded).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d68f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    \n",
    "    # Initializes an ANN with specified architecture, bias, learning rate, and creates initial weights.\n",
    "    \n",
    "    def __init__(self,no_inputs, hidden_layers, no_outputs,bias,learning_rate):\n",
    "        \n",
    "        self.no_inputs=no_inputs\n",
    "        self.hidden_layers=hidden_layers # List indicating the number of neurons in each hidden layer.\n",
    "        self.no_outputs=no_outputs\n",
    "        self.bias = bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.structure = [no_inputs] + hidden_layers + [no_outputs] # Full network structure including input, hidden, and output layers.\n",
    "        self.create_weight() # Calls function to initialize weights based on the network structure.\n",
    "    \n",
    "    \n",
    "    # Creates and initializes weights for each layer in the network.\n",
    "    def create_weight(self):\n",
    "        bias_node = 1 if self.bias else 0\n",
    "        self.len_network = len(self.structure) # Length of the structure list indicating the number of layers.\n",
    "        layer_index = 0 # Index to iterate through layers.\n",
    "        self.weights = []  # List to hold weight matrices.\n",
    "        \n",
    "        # Iterates through the layers to create weight matrices between them.\n",
    "        \n",
    "        while layer_index < self.len_network - 1:\n",
    "            nodes_out = self.structure[layer_index + 1]\n",
    "            nodes_in = self.structure[layer_index]\n",
    "            n = nodes_out * (nodes_in + bias_node) # Total number of weights needed.\n",
    "            rad = 1 / np.sqrt(nodes_in)  # Radial basis for weight initialization range.\n",
    "            X=truncated_normal(low=-rad,upp=rad,mean=0,sd=1) # Truncated normal distribution for weight initialization.\n",
    "            wm = X.rvs(n).reshape((nodes_out, nodes_in + 1)) # Reshaping the randomly sampled weights.\n",
    "            self.weights.append(wm)\n",
    "            layer_index += 1\n",
    "        return self.weights\n",
    "    \n",
    "    # Trains the network using batch learning over a specified number of epochs.\n",
    "    \n",
    "    def train_batch(self, X_train, labels, epochs, batch_size):\n",
    "        self.combined_weights = []  # List to store weights after each batch update for analysis.\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Loop over each batch within the dataset.\n",
    "            \n",
    "            for start_idx in range(0, X_train.shape[0], batch_size):\n",
    "                end_idx = min(start_idx + batch_size, X_train.shape[0]) # Ensures the batch does not exceed dataset size.\n",
    "                batch_x = X_train[start_idx:end_idx] # Extracts features for the current batch.\n",
    "                batch_y = labels[start_idx:end_idx] # Extracts labels for the current batch.\n",
    "\n",
    "                result_vector = [batch_x.T] # Prepares the input vector for the forward pass\n",
    "                layer_index = 0 # Sets layer index for forward pass.\n",
    "\n",
    "                while layer_index < self.len_network - 1:\n",
    "                    input_vector = result_vector[-1] # Takes the last layer's output as the current input.\n",
    "\n",
    "                    if self.bias:  # Adds bias to the input vector if bias is enabled.\n",
    "                        bias_terms = np.ones((1, input_vector.shape[1])) * self.bias\n",
    "                        input_vector = np.concatenate((input_vector, bias_terms), axis=0)\n",
    "                        result_vector[-1] = input_vector\n",
    "                    \n",
    "                    output_z = np.dot(self.weights[layer_index], input_vector)\n",
    "                    output_activation = activate(output_z)\n",
    "                    result_vector.append(output_activation)\n",
    "                    layer_index += 1\n",
    "\n",
    "                output_error = batch_y.T - result_vector[-1] # Computes error at the output layer.\n",
    "                \n",
    "                backward_index = self.len_network - 1 # Sets backward index for backpropagation.\n",
    "                \n",
    "                while backward_index > 0:\n",
    "                    \n",
    "                    output_activation = result_vector[backward_index]  # Activation of current layer.\n",
    "                    input_activation = result_vector[backward_index - 1] # Activation of previous layer\n",
    "                    \n",
    "                    # Adjusts the activation for bias nodes not in the output layer.\n",
    "                    \n",
    "                    if self.bias and backward_index!=self.len_network - 1 and self.structure[-1]>1:\n",
    "                        \n",
    "                        output_activation=output_activation.copy()[:-1,:]\n",
    "                    \n",
    "                    # Applies derivative of the activation function for non-output layers.\n",
    "                        \n",
    "                    if self.structure[-1]>1:\n",
    "                \n",
    "                        output_error=output_error*output_activation*(1-output_activation)\n",
    "                    \n",
    "                    gradient = np.dot(output_error, input_activation.T) / batch_x.shape[0]\n",
    "\n",
    "                    self.weights[backward_index - 1] += self.learning_rate * gradient # Updates weights.\n",
    "                    \n",
    "                    output_error = np.dot(self.weights[backward_index - 1].T, output_error) # Calculates error for previous layer.\n",
    "\n",
    "                    # Adjusts error for bias nodes.\n",
    "\n",
    "                    if self.bias: \n",
    "                        \n",
    "                        output_error = output_error[:-1, :]\n",
    "                    \n",
    "                    backward_index -= 1 # Moves to the previous layer.\n",
    "                \n",
    "                self.combined_weights.append(self.weights) # Stores updated weights after each batch.\n",
    "        return self.combined_weights   # Returns weights after training.\n",
    "\n",
    "    def predict(self, input_vector): #this repeats the forward propagation of the above-explained training process to predict\n",
    "        \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        result_vector = [input_vector]\n",
    "        layer_index = 0\n",
    "\n",
    "        while layer_index < self.len_network - 1:\n",
    "            input_vector = result_vector[-1]\n",
    "\n",
    "            if self.bias:\n",
    "                input_vector = np.concatenate((input_vector, [[self.bias]]))\n",
    "                result_vector[-1] = input_vector\n",
    "\n",
    "            output_z = np.dot(self.weights[layer_index], input_vector)\n",
    "            output_activation = activate(output_z)\n",
    "            result_vector.append(output_activation)\n",
    "            layer_index += 1\n",
    "        \n",
    "        return output_activation\n",
    "        \n",
    "    # Calculates accuracy of the model on a given dataset.\n",
    "    def accuracy(self, data_array, labels):\n",
    "        correct, wrong = 0, 0 # Initializes counters for correct and incorrect predictions\n",
    "\n",
    "        # Iterates over the dataset to predict and compare with actual labels.\n",
    "        \n",
    "        for i in range(len(data_array)):\n",
    "            \n",
    "            # For multi-class classification, uses argmax to find predicted class.\n",
    "            \n",
    "            if self.structure[-1]>1:\n",
    "                \n",
    "                predicted=self.predict(data_array[i]) # Predicts based on input.\n",
    "                predicted_max=np.argmax(predicted) # Finds class with highest probability.\n",
    "                actual_label=labels[i]  # Actual class label.\n",
    "                \n",
    "                 # Updates counters based on prediction accuracy.\n",
    "                    \n",
    "                if predicted_max==actual_label:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    wrong += 1\n",
    "                accuracy=correct/(correct+wrong)\n",
    "            \n",
    "            # For binary classification, thresholds the predicted value.\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                predicted = self.predict(data_array[i])\n",
    "                predicted = np.where(predicted[0][0] >= 0.5, 1, 0) # Applies threshold to prediction.\n",
    "                actual_label = labels[i]\n",
    "                \n",
    "                \n",
    "                # Updates counters based on prediction accuracy.\n",
    "\n",
    "                if predicted == actual_label:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    wrong += 1\n",
    "                accuracy=correct/(correct+wrong)\n",
    "\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "    def confusion_matrix(self, data_array, labels): # Generates a confusion matrix for the model predictions on a given dataset\n",
    "        \n",
    "        length=len(np.unique(labels))  # Determines size of the confusion matrix based on number of unique labels.\n",
    "        \n",
    "        cm=np.zeros((length,length),int) # Initializes the confusion matrix with zeros.\n",
    "        \n",
    "        # Iterates over the dataset to fill the confusion matrix.\n",
    "        \n",
    "        for i in range(len(data_array)):\n",
    "            \n",
    "            # For multi-class classification, identifies the predicted class and actual class.\n",
    "            \n",
    "            if self.structure[-1]>1:\n",
    "\n",
    "                predicted=self.predict(data_array[i])\n",
    "                predicted_max=np.argmax(predicted) # Finds class with highest probability.\n",
    "                actual_label=int(labels[i])\n",
    "                cm[actual_label,predicted_max]+=1  # Updates confusion matrix.\n",
    "\n",
    "            else:\n",
    "\n",
    "                predicted = self.predict(data_array[i])\n",
    "                predicted = np.where(predicted >= 0.15, 1, 0)\n",
    "                actual_label = int(labels[i])\n",
    "                cm[actual_label, predicted] += 1\n",
    "\n",
    "        return cm\n",
    "\n",
    "    # Calculates precision and recall for a given class label based on the confusion matrix.\n",
    "   \n",
    "    def precision_recall(self, label, confusion_matrix):\n",
    "        \n",
    "        # For multi-class classification, calculates precision and recall for the specified label.\n",
    "        \n",
    "        if self.structure[-1]>1:\n",
    "            \n",
    "            true_positives=confusion_matrix[label,label]\n",
    "\n",
    "            true_positive_plus_false_positives=confusion_matrix[:,label].sum()\n",
    "\n",
    "            true_positives_plus_false_negatives=confusion_matrix[label,:].sum()\n",
    "            \n",
    "            precision=true_positives/true_positive_plus_false_positives\n",
    "\n",
    "            recall=true_positives/true_positives_plus_false_negatives\n",
    "\n",
    "            \n",
    "         # For binary classification, simplifies precision and recall calculation.\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            true_positives=confusion_matrix[0,0]+confusion_matrix[1,1]\n",
    "            false_positives=confusion_matrix[0,1]\n",
    "            false_negatives=confusion_matrix[1,0]\n",
    "        \n",
    "            precision=true_positives/(true_positives+false_positives)\n",
    "            recall=true_positives/(true_positives+false_negatives)\n",
    "     \n",
    "        return precision,recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4de48c",
   "metadata": {},
   "source": [
    "### Testing ANN for Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfab6cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 55.484747886657715 seconds\n"
     ]
    }
   ],
   "source": [
    "start_mutliclass_training_time = time.time()\n",
    "multiclass_sigmoid=ANN(784,[3,4,5],10,1,0.5)\n",
    "multiclass_weights=multiclass_sigmoid.train_batch(X_train,y_train_encoded,100,64)\n",
    "end_mutliclass_training_time = time.time()\n",
    "time_taken_multiclass_training=end_mutliclass_training_time-start_mutliclass_training_time\n",
    "print(f\"Execution time: {time_taken_multiclass_training} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b36f177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the multiclass classification is 0.3530516431924883\n"
     ]
    }
   ],
   "source": [
    "accuracy_multiclass=multiclass_sigmoid.accuracy(X_test,y_test)\n",
    "print(f\"The accuracy for the multiclass classification is {accuracy_multiclass}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49152bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[518 123   4  35  13  32  38  60   8  65]\n",
      " [117 248  17  83  72  30 189  77   7  48]\n",
      " [ 10  38 133 131   5 258  63   1   7  10]\n",
      " [  4  21  87 293  28 165 257   0  12  13]\n",
      " [ 11 125  48 168 135  76 289   2  17  25]\n",
      " [ 18  17  66 112  11 629  21   1   0  13]\n",
      " [ 17  30  22 268 126  38 276   0   7  16]\n",
      " [122  48  11  35   8  24  48 495  10  87]\n",
      " [  4 110  24 213 131  47 268   0  15  28]\n",
      " [103  77  34  90  43 155  54  45  21 266]]\n"
     ]
    }
   ],
   "source": [
    "multiclass_confusion_matrix=multiclass_sigmoid.confusion_matrix(X_test,y_test)\n",
    "print(multiclass_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ccc7959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for label 0 is 0.5606060606060606,while recall for label 0 is 0.578125\n",
      "precision for label 1 is 0.2962962962962963,while recall for label 1 is 0.27927927927927926\n",
      "precision for label 2 is 0.2982062780269058,while recall for label 2 is 0.2027439024390244\n",
      "precision for label 3 is 0.20518207282913165,while recall for label 3 is 0.33295454545454545\n",
      "precision for label 4 is 0.23601398601398602,while recall for label 4 is 0.15066964285714285\n",
      "precision for label 5 is 0.4325997248968363,while recall for label 5 is 0.7083333333333334\n",
      "precision for label 6 is 0.18363273453093812,while recall for label 6 is 0.345\n",
      "precision for label 7 is 0.7268722466960352,while recall for label 7 is 0.5574324324324325\n",
      "precision for label 8 is 0.14423076923076922,while recall for label 8 is 0.017857142857142856\n",
      "precision for label 9 is 0.4658493870402802,while recall for label 9 is 0.29954954954954954\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    multiclass_precision,multiclass_recall=multiclass_sigmoid.precision_recall(i,multiclass_confusion_matrix)\n",
    "    print(f\"precision for label {i} is {multiclass_precision},while recall for label {i} is {multiclass_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0751a0dc",
   "metadata": {},
   "source": [
    "### Testing ANN for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af3d071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 46.184836864471436 seconds\n"
     ]
    }
   ],
   "source": [
    "start_binary_training_time = time.time()\n",
    "\n",
    "binary_sigmoid=ANN(784,[3,4,5],1,1,0.5)\n",
    "\n",
    "binary_weights=binary_sigmoid.train_batch(X_train,class7_train,100,64)\n",
    "\n",
    "end_binary_training_time = time.time()\n",
    "\n",
    "time_taken_binary_training=end_binary_training_time - start_binary_training_time\n",
    "\n",
    "print(f\"Execution time: {time_taken_binary_training} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1c24088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for binary classification on Label 7--Ship-- is 0.895774647887324\n"
     ]
    }
   ],
   "source": [
    "accuracy_binary=binary_sigmoid.accuracy(X_test,class7_test)\n",
    "print(f\"Accuracy score for binary classification on Label 7--Ship-- is {accuracy_binary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c030b9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7632,    0],\n",
       "       [ 888,    0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_confusion_matrix=binary_sigmoid.confusion_matrix(X_test,class7_test)\n",
    "binary_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd9cc3",
   "metadata": {},
   "source": [
    "## Updating the ANN to use the ReLU activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c2f8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "class ANN_RELU:\n",
    "    \n",
    "    # Initialize an ANN instance with specified architecture, bias, and learning rate.\n",
    "    \n",
    "    def __init__(self,no_inputs, hidden_layers, no_outputs,bias,learning_rate):\n",
    "        \n",
    "        self.bias = bias \n",
    "        self.learning_rate = learning_rate\n",
    "        self.no_outputs=no_outputs\n",
    "        self.no_inputs=no_inputs\n",
    "        self.hidden_layers=hidden_layers\n",
    "        self.structure = [no_inputs] + hidden_layers + [no_outputs] # Complete network structure.\n",
    "        self.create_weight() # Call the method to initialize the weights of the network.\n",
    "        \n",
    "\n",
    "    # Method to create and initialize the weights of the network.\n",
    "    \n",
    "    def create_weight(self):\n",
    "        bias_node = 1 if self.bias else 0    # Determine if a bias node should be added (1) or not (0).\n",
    "        self.len_network = len(self.structure)\n",
    "        layer_index = 0  # Start index for iterating through layers.\n",
    "        self.weights = []  # Initialize an empty list to store the weight matrices\n",
    "        \n",
    "        # Loop through each layer to initialize weights until the second last layer.\n",
    "        \n",
    "        while layer_index < self.len_network - 1:\n",
    "            \n",
    "            nodes_out = self.structure[layer_index + 1] # Number of nodes in the next layer.\n",
    "            \n",
    "            nodes_in = self.structure[layer_index]\n",
    "            \n",
    "            n = nodes_out * (nodes_in + bias_node) # Total number of weights to initialize for the layer\n",
    "            \n",
    "            rad = 1 / np.sqrt(nodes_in)  # Scaling factor for weight initialization.\n",
    "            \n",
    "            X=truncated_normal(low=-rad,upp=rad,mean=0,sd=1) # Create a truncated normal distribution for weight initialization.\n",
    "            \n",
    "            wm = X.rvs(n).reshape((nodes_out, nodes_in + 1)) # Randomly sample weights and reshape into a matrix.\n",
    "            \n",
    "            self.weights.append(wm) # Append the weight matrix to the weights list.\n",
    "            \n",
    "            layer_index += 1 # Move to the next layer index.\n",
    "            \n",
    "        return self.weights\n",
    "    \n",
    "    \n",
    "    def alternative_weight_init(self):\n",
    "    \n",
    "    \n",
    "    #This weight initialization demonstrates an alternative approach to initializing weights for the neural network\n",
    "    \n",
    "    #Directly alligned with the pseudocode in the template\n",
    "    \n",
    "    # TO-DO:\n",
    "        #for layer in range(len(hidden_layers)):\n",
    "            #no_nodes = hidden_layers[layer]\n",
    "            # no_inputs_to_layer = ??\n",
    "            # initialise weight matrix of shape: (no_nodes, no_inputs_to_layer)\n",
    "  \n",
    "    \n",
    "        bias_node=1 if self.bias else 0\n",
    "    \n",
    "        self.weights_new=[]\n",
    "    \n",
    "        for layer in range(len(self.hidden_layers)+1):\n",
    "        \n",
    "            nodes_in=self.hidden_layers[layer-1]\n",
    "        \n",
    "            if layer==0:\n",
    "                nodes_in=self.no_inputs+bias_node\n",
    "            \n",
    "            if layer==3:\n",
    "                nodes_out=self.no_outputs\n",
    "            else:\n",
    "                nodes_out=self.hidden_layers[layer]\n",
    "\n",
    "            n = nodes_out * (nodes_in + bias_node)\n",
    "\n",
    "            rad = 1 / np.sqrt(nodes_in)\n",
    "\n",
    "            X=truncated_normal(low=-rad,upp=rad,mean=0,sd=1)\n",
    "\n",
    "            wm = X.rvs(n).reshape((nodes_out, nodes_in + 1))\n",
    "\n",
    "            self.weights_new.append(wm)\n",
    "\n",
    "            layer += 1\n",
    "        \n",
    "        return self.weights_new\n",
    "\n",
    "    \n",
    "    def train_batch(self, X_train, labels, epochs, batch_size):\n",
    "        \n",
    "        self.combined_weights = []  # Initialize a list to store weights after each update.\n",
    "\n",
    "        for epoch in range(epochs): # Iterate over each epoch.\n",
    "            \n",
    "            # Iterate over batches of the training data.\n",
    "            \n",
    "            for start_idx in range(0, X_train.shape[0], batch_size): \n",
    "                end_idx = min(start_idx + batch_size, X_train.shape[0]) # Determine the end index of the batch.\n",
    "                batch_x = X_train[start_idx:end_idx] # Extract the batch of features.\n",
    "                batch_y = labels[start_idx:end_idx] # Extract the batch of labels.\n",
    "\n",
    "                result_vector = [batch_x.T] # Initialize the result vector with the batch's input.\n",
    "                layer_index = 0 # Index to iterate through layers.\n",
    "\n",
    "                # Forward propagation\n",
    "                \n",
    "                while layer_index < self.len_network - 1:\n",
    "                    input_vector = result_vector[-1]\n",
    "\n",
    "                    if self.bias:\n",
    "                        bias_terms = np.ones((1, input_vector.shape[1])) * self.bias\n",
    "                        input_vector = np.concatenate((input_vector, bias_terms), axis=0)\n",
    "                        result_vector[-1] = input_vector\n",
    "                    \n",
    "                    output_z = np.dot(self.weights[layer_index], input_vector)\n",
    "                    output_activation = relu_activation(output_z)\n",
    "                    result_vector.append(output_activation)\n",
    "                    layer_index += 1\n",
    "                \n",
    "                output_error = batch_y.T - result_vector[-1]  # Calculate the output error.\n",
    "                \n",
    "                # Backpropagation\n",
    "                \n",
    "                backward_index = self.len_network - 1 # Start from the last layer.\n",
    "                \n",
    "                while backward_index > 0:\n",
    "                    \n",
    "                    output_activation = result_vector[backward_index] # Get the activation for the current layer.\n",
    "                    input_activation = result_vector[backward_index - 1] # Get the input activation.\n",
    "                    \n",
    "                    # Remove bias terms from the output activation if not in the output layer.\n",
    "                    \n",
    "                    if self.bias and backward_index!=self.len_network - 1 and self.structure[-1]>1:\n",
    "                        \n",
    "                        output_activation=output_activation.copy()[:-1,:]\n",
    "                        \n",
    "                    # Apply derivative of activation function if not output layer.\n",
    "                    \n",
    "                    if self.structure[-1]>1:\n",
    "                        \n",
    "                        output_error=output_error*output_activation*(1-output_activation)\n",
    "        \n",
    "                    relu_derivative=np.where(output_activation<=0,0,1)  # Derivative of ReLU.\n",
    "                \n",
    "                    output_error=output_error *relu_derivative\n",
    "                    \n",
    "                    \n",
    "                    # Gradient calculation with clipping to prevent exploding gradients.\n",
    "                        \n",
    "                    clip_value = 1.0  # to prevent incompatible gradients\n",
    "                    \n",
    "                    gradient = np.dot(output_error, input_activation.T) / batch_x.shape[0]\n",
    "                \n",
    "                    gradient = np.clip(gradient, -clip_value, clip_value)\n",
    "                    \n",
    "                    # Check for NaN values in the weights.\n",
    "\n",
    "                    self.weights[backward_index - 1] += self.learning_rate * gradient\n",
    "                \n",
    "    \n",
    "                    for weight_matrix in self.weights:\n",
    "                        assert not np.isnan(weight_matrix).any(), \"Weights have NaN values\"\n",
    "\n",
    "                    # Update the error for the previous layer.\n",
    "                    \n",
    "                    output_error = np.dot(self.weights[backward_index - 1].T, output_error)\n",
    "    \n",
    "\n",
    "                    if self.bias: \n",
    "                        \n",
    "                        output_error = output_error[:-1, :]\n",
    "                    \n",
    "                    \n",
    "                    backward_index -= 1\n",
    "                \n",
    "                \n",
    "                \n",
    "                self.combined_weights.append(self.weights)\n",
    "                \n",
    "                \n",
    "        return self.combined_weights\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        result_vector = [input_vector]\n",
    "        layer_index = 0\n",
    "\n",
    "        while layer_index < self.len_network - 1:\n",
    "            input_vector = result_vector[-1]\n",
    "\n",
    "            if self.bias:\n",
    "                input_vector = np.concatenate((input_vector, [[self.bias]]))\n",
    "                result_vector[-1] = input_vector\n",
    "\n",
    "            output_z = np.dot(self.weights[layer_index], input_vector)\n",
    "            output_activation = relu_activation(output_z)\n",
    "            result_vector.append(output_activation)\n",
    "            layer_index += 1\n",
    "        \n",
    "        return output_activation\n",
    "        \n",
    "    def accuracy(self, data_array, labels):\n",
    "        correct, wrong = 0, 0 # Initialize counters for correct and wrong predictions.\n",
    "\n",
    "        for i in range(len(data_array)):\n",
    "            \n",
    "            # For multi-class classification.\n",
    "            \n",
    "            if self.structure[-1]>1:\n",
    "                \n",
    "                predicted=self.predict(data_array[i]) # Get the model's prediction.\n",
    "                predicted_max=np.argmax(predicted) # Find the class with the highest probability\n",
    "                actual_label=labels[i] # Get the actual label\n",
    "                \n",
    "                # Increment the correct or wrong counter based on the prediction accuracy\n",
    "                \n",
    "                if predicted_max==actual_label:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    wrong += 1\n",
    "                    \n",
    "                # Calculate accuracy as the ratio of correct predictions to total predictions.\n",
    "                \n",
    "                accuracy=correct/(correct+wrong)\n",
    "            \n",
    "              \n",
    "            else:\n",
    "                \n",
    "                predicted = self.predict(data_array[i]) # Get the model's prediction.\n",
    "                predicted = np.where(predicted[0][0] >= 0.5, 1, 0)  # Threshold the prediction at 0.5.\n",
    "                actual_label = labels[i]\n",
    "\n",
    "                if predicted == actual_label:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    wrong += 1\n",
    "                accuracy=correct/(correct+wrong)\n",
    "\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "   \n",
    "    def confusion_matrix(self, data_array, labels):\n",
    "        \n",
    "        length=len(np.unique(labels)) # Determine the size of the confusion matrix\n",
    "        \n",
    "        cm=np.zeros((length,length),int) # Initialize the confusion matrix with zeros\n",
    "        \n",
    "        # Iterate over each example in the dataset\n",
    "        \n",
    "        for i in range(len(data_array)):\n",
    "            \n",
    "            # For multi-class classification.\n",
    "            \n",
    "            if self.structure[-1]>1:\n",
    "\n",
    "                predicted=self.predict(data_array[i])\n",
    "                predicted_max=np.argmax(predicted)\n",
    "                actual_label=int(labels[i])\n",
    "                cm[actual_label,predicted_max]+=1 # Increment the corresponding cell in the confusion matrix\n",
    "\n",
    "            else:\n",
    "\n",
    "                predicted = self.predict(data_array[i])\n",
    "                predicted = np.where(predicted >= 0.15, 1, 0)\n",
    "                actual_label = int(labels[i])\n",
    "                cm[actual_label, predicted] += 1\n",
    "\n",
    "        return cm\n",
    "    \n",
    "    \n",
    "  \n",
    "    def precision_recall(self, label, confusion_matrix):\n",
    "        \n",
    "    # For multi-class classification.\n",
    "        if self.structure[-1] > 1:\n",
    "\n",
    "            true_positives = confusion_matrix[label, label]  # Get true positives for the given label.\n",
    "\n",
    "            # Calculate the sum of the column for the given label to get true positives plus false positives.\n",
    "            true_positive_plus_false_positives = confusion_matrix[:, label].sum()\n",
    "\n",
    "            # Calculate the sum of the row for the given label to get true positives plus false negatives.\n",
    "            true_positives_plus_false_negatives = confusion_matrix[label, :].sum()\n",
    "\n",
    "            # Calculate precision and recall for the given label.\n",
    "            precision = true_positives / true_positive_plus_false_positives\n",
    "            recall = true_positives / true_positives_plus_false_negatives\n",
    "\n",
    "        # For binary classification.\n",
    "        else:\n",
    "\n",
    "            true_positives = confusion_matrix[0, 0] + confusion_matrix[1, 1]  # Sum of the diagonal elements for true positives.\n",
    "            false_positives = confusion_matrix[0, 1]  # False positives.\n",
    "            false_negatives = confusion_matrix[1, 0]  # False negatives.\n",
    "\n",
    "            # Calculate precision and recall for binary classification.\n",
    "            precision = true_positives / (true_positives + false_positives)\n",
    "            recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        return precision, recall  # Return the calculated precision and recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "124362be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 360.53876781463623 seconds\n",
      "Accuracy for RELU is 0.16490610328638497\n"
     ]
    }
   ],
   "source": [
    "start_RELU_training_time = time.time()\n",
    "\n",
    "relu_neural=ANN_RELU(784,[100,80,50],10,1,0.001)\n",
    "\n",
    "relu_weights=relu_neural.train_batch(X_train,y_train_encoded,100,64)\n",
    "\n",
    "end_RELU_training_time = time.time()\n",
    "\n",
    "time_taken_RELU_training=end_RELU_training_time - start_RELU_training_time\n",
    "\n",
    "print(f\"Execution time: {time_taken_RELU_training} seconds\")\n",
    "\n",
    "accuracy_relu=relu_neural.accuracy(X_test,y_test)\n",
    "print(f\"Accuracy for RELU is {accuracy_relu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d96dc00",
   "metadata": {},
   "source": [
    "## Analysis for sigmoid neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db0af4",
   "metadata": {},
   "source": [
    "●How much better are the results for object recognition, compared to the single-layer perceptron? \n",
    "\n",
    "Interestingly, the sigmoid neural network performed closely in accuracy (accuracy of 89.56%) to the single-perceptron sigmoid (accuracy of 89.87%). However, the sigmoid neural network took far more time to train.\n",
    "\n",
    "\n",
    "● How did you modify the initial weights, learning rate, and iterations to achieve this? \n",
    "\n",
    "I significantly increased the number of nodes in the hidden network to improve learning and avoid overfitting, enabling the model to better generalize to unseen data.\n",
    "\n",
    "● How much faster/slower is the training time, compared to the single-layer perceptron? \n",
    "\n",
    "Took far more training time: 285.80098581314087 seconds\n",
    "\n",
    "● How much quicker/slower does the learning converge, compared to the single-layer perceptron?  The single-layer perceptron converged way quicker than its multi-layer sigmoid equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861fe58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
